{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:nlp]",
      "language": "python",
      "name": "conda-env-nlp-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "1.1 Spacy - an introduction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tongni1975/100-Days-Of-ML-Code/blob/master/1_1_Spacy_an_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDH3zLByaIyt",
        "colab_type": "text"
      },
      "source": [
        "# Spacy basic NLP tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5olwg2zPaIyv",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we introduce basic NLP tasks that are associated with Spacy. Spacy is selected as it is an industrial-grade NLP library that is known to be fastest and most accurate in the industry and research. \n",
        "\n",
        "The reference for this tutorial is from https://spacy.io/usage/. Some of the material is obtained from the spacy notebooks therein.\n",
        "\n",
        "The Spacy library is described as 4 sections - usage, models, api and universe. The set-up instructions are in the usage, while the model section looks into the models for tagging, parsing and entity recognition. There are no inherent models to do training for sentiment etc, which still need to call on libraries like sci-kit learn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81iOAjYOaIyw",
        "colab_type": "text"
      },
      "source": [
        "## Architecture\n",
        "The Spacy library is useful as a building block to build NLP libraries - a valuable skillset. To do so, a basic understanding of its architecture is necessary. A pictorial of it below. Spacy comes with models for its 'Language' models - which are used to build the 'Vocab' that is particular to your use case. Further, the 'Doc' as represented below are your own training or test documents. The Doc is broken into tokens (words or groups of words), span (a subset of documents) or lexeme (refers to the root words of words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PWtFrsoaIyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "531495b6-41ec-4cac-e548-adbe38c3f23b"
      },
      "source": [
        "from os import chdir, getcwd\n",
        "fpath = getcwd()\n",
        "print (fpath)\n",
        "from IPython.display import Image\n",
        "Image(filename=fpath + '\\\\images\\\\architecture.PNG', width=550, height=400)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-99d167a1a400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\\\images\\\\architecture.PNG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m550\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content\\\\images\\\\architecture.PNG'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMNUr5W-aIy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c2ca474-fa09-4d84-d0e5-cced1522c3af"
      },
      "source": [
        "# this is to make sure we get no unicode based errors\n",
        "from __future__ import unicode_literals\n",
        "import spacy\n",
        "import numpy as np\n",
        "print (spacy.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibLtdA03avPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "fb5b86d1-3854-46e8-c8b4-8bbb60967336"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlpWJhxQaIy3",
        "colab_type": "text"
      },
      "source": [
        "## Containers in Spacy\n",
        "There are 4 key containers in spacy - \n",
        "1. Tokens - they are simplest and contain many useful different properties.\n",
        "2. doc - this is a sequence of tokens\n",
        "3. span - this is a slice from a doc object \n",
        "4. lexme - an entry in the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaDRjXhWaIy4",
        "colab_type": "text"
      },
      "source": [
        "First we load the spacy Language model (English). In order to process sentences, we need to use the model 'nlp', which does parsing, POS, NER on the sentence on each of the tokens. We explore the properites of these tokens.\n",
        "\n",
        "## Tokens\n",
        "\n",
        "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\".\n",
        "\n",
        "Example: The following sentence can be tokenised by splitting up the sentence into individual words.\n",
        "\n",
        "\t\"Cytora is going to PyCon!\"\n",
        "\t[\"Cytora\",\"is\",\"going\",\"to\",\"PyCon!\"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epkjPORMaIy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "570a9412-fa3c-4fbd-90fd-ccbc1a3da114"
      },
      "source": [
        "# u need to run for the spacy models\n",
        "# python -m spacy download en_core_web_sm or md\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'Hello world! Natural Language Processing 101 is f8unn')\n",
        "\n",
        "# Get first token of the processed document\n",
        "token = doc[0]\n",
        "print(token)\n",
        "\n",
        "# Print sentences (one sentence per line)\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "Hello world!\n",
            "Natural Language Processing 101 is f8unn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNTP83TMaIy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72ddc874-c341-42c1-979c-b0ea20e08d4a"
      },
      "source": [
        "print(token.orth)  # token uinque id. These are stored as hashes ID/\n",
        "print(token.orth_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15777305708150031551\n",
            "Hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSHh1nytaIy_",
        "colab_type": "text"
      },
      "source": [
        "### Token tags\n",
        "\n",
        "A POS tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
        "More information about the kinds of speech tags which are used in Spacy can be [found here](https://spacy.io/api/annotation#section-pos-tagging).\n",
        "\n",
        "Examples:\n",
        "\n",
        "1. CARDINAL, Cardinal Number - 1,2,3\n",
        "2. PROPN, Proper Noun, Singular - \"Matic\", \"Andraz\", \"Cardiff\"\n",
        "3. INTJ, Interjection - \"Uhhhhhhhhhhh\"\n",
        "\n",
        "The following are some tags associated with tokens:\n",
        "- Text: The original word text.\n",
        "- Lemma: The base form of the word.\n",
        "- POS: The simple part-of-speech tag.\n",
        "- Tag: The detailed part-of-speech tag.\n",
        "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "- Shape: The word shape – capitalization, punctuation, digits.\n",
        "- is alpha: Is the token an alpha character?\n",
        "- is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\n",
        "For more properties, refer to https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woaKunMpaIzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8c7a4749-7b96-4cad-f97f-6287bb9b86ea"
      },
      "source": [
        "# For each token, print corresponding part of speech tag\n",
        "for token in doc:\n",
        "    print('{} - {}'.format(token, token.pos_), token.has_vector, token.vector_norm, token.is_oov)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello - INTJ True 24.419973 True\n",
            "world - NOUN True 22.475723 True\n",
            "! - PUNCT True 25.938637 True\n",
            "Natural - PROPN True 23.645964 True\n",
            "Language - PROPN True 24.362488 True\n",
            "Processing - PROPN True 25.772451 True\n",
            "101 - NUM True 24.622292 True\n",
            "is - VERB True 26.335238 True\n",
            "f8unn - PUNCT True 20.704897 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYuq4rrXaIzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d3ed4b70-4e75-4001-bd0e-a80e8b388dab"
      },
      "source": [
        "from spacy.parts_of_speech import ADV\n",
        "\n",
        "def is_adverb(token):\n",
        "    return token.pos == spacy.parts_of_speech.ADV\n",
        "\n",
        "NNS = nlp.vocab.strings['NNS']\n",
        "NNPS = nlp.vocab.strings['NNPS']\n",
        "def is_plural_noun(token):\n",
        "    return token.tag == NNS or token.tag == NNPS\n",
        "    \n",
        "doc = nlp(u\"Apples and orange are similar.\")\n",
        "for token in doc:\n",
        "    print(token, token.pos_, token.tag_, is_adverb(token), is_plural_noun(token), token.sentiment)\n",
        "# for more properties, refer to https://spacy.io/api/token"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apples NOUN NNS False True 0.0\n",
            "and CCONJ CC False False 0.0\n",
            "orange NOUN NN False False 0.0\n",
            "are VERB VBP False False 0.0\n",
            "similar ADJ JJ False False 0.0\n",
            ". PUNCT . False False 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5XnqBepaIzG",
        "colab_type": "text"
      },
      "source": [
        "### Token unigram probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kr69-9NaIzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1a6c9176-703d-4830-bd74-facb04ee9b6c"
      },
      "source": [
        "# For every token in doc, print log-probability of the word, estimated from counts from the reference model Need to use a larger model to observe\n",
        "for token in doc:\n",
        "    print(token, ',', np.exp(token.prob))                                     #how often does the word occur"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apples , 2.061153622438558e-09\n",
            "and , 2.061153622438558e-09\n",
            "orange , 2.061153622438558e-09\n",
            "are , 2.061153622438558e-09\n",
            "similar , 2.061153622438558e-09\n",
            ". , 2.061153622438558e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RljI5UShaIzI",
        "colab_type": "text"
      },
      "source": [
        "## Doc object\n",
        "\n",
        "From the doc, it is possible to obtain named entities, which is any real world object such as a person, location, organisation or product with a proper name. The doc object also has noun chunks. Noun chunks are the phrases based upon nouns recovered from tokenized text using the speech tags.\n",
        "\n",
        "Example:\n",
        "\n",
        "\t1. Barack Obama\n",
        "\t2. Edinburgh\n",
        "\t3. Ferrari Enzo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gb6KsbGaIzJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f0cbc185-99f6-416a-dced-1242fd49c370"
      },
      "source": [
        "# Print all named entities with named entity types\n",
        "doc_2 = nlp(u\"Barack Obama lived in Washington and liked to eat Burger King.\")  # ths create a doc object which has ents\n",
        "for ent in doc_2.ents:\n",
        "    print('{} - {}'.format(ent, ent.label_))\n",
        "    \n",
        "doc_3 = nlp(u\"The boy loves to play football.\") \n",
        "print([chunk for chunk in doc_3.noun_chunks])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Barack Obama - PERSON\n",
            "Washington - GPE\n",
            "Burger King - ORG\n",
            "[The boy, football]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS_mmHefaIzM",
        "colab_type": "text"
      },
      "source": [
        "#### Export to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mqDWTy2aIzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.attrs import ORTH, LIKE_URL, IS_OOV\n",
        "\n",
        "attr_ids = [ORTH, LIKE_URL, IS_OOV]\n",
        "doc_array = doc_2.to_array(attr_ids)\n",
        "assert doc_array.shape == (len(doc_2), len(attr_ids))\n",
        "assert doc_2[0].orth == doc_array[0, 0]\n",
        "assert doc_2[1].orth == doc_array[1, 0]\n",
        "assert doc_2[0].like_url == doc_array[0, 1]\n",
        "assert list(doc_array[:, 1]) == [t.like_url for t in doc_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TPVwovaIzQ",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary\n",
        "A vocabulary is a collection of lexeme - which are the root of words. It is also possible to extract all words from the built-in vocabulary, but memory will overflow below. The vocabulary is useful in builing training data sets. Here we create a new vocabulary from some words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp5p94IkaIzR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "23a74a1b-a47b-4089-a002-438b5c85c0bf"
      },
      "source": [
        "# Vocabulary from spacy model\n",
        "apple_id = nlp.vocab.strings['apple']   # nlp is the language model\n",
        "oov = nlp.vocab.strings['dskfodkfos']\n",
        "\n",
        "print (apple)  # the words are identified as hashes in the vocabulary\n",
        "print (oov)\n",
        "\n",
        "apple_str = nlp.vocab.strings[apple_id]\n",
        "print (apple_str)\n",
        "assert oov not in nlp.vocab\n",
        "\n",
        "# Create new vocabulary of two words\n",
        "from spacy.vocab import Vocab\n",
        "new_vocab = Vocab(strings=[u'hello', u'world'])   \n",
        "for i in new_vocab:\n",
        "    print(i.text, i.orth)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bb3d92a4210f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dskfodkfos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mapple\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the words are identified as hashes in the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'apple' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn3cZc-JaIzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a06f928c-9db1-4a41-a0b1-6c29fd69e74b"
      },
      "source": [
        "# printing out some of the stop words in the model vocabulary\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "print(STOP_WORDS) # <- set of Spacy's default stop words\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'serious', 'between', 'he', 'us', 'used', 'whereas', 'hers', 'only', 'until', 'your', 'hundred', '‘s', 'hereafter', 'thereafter', 'so', 'can', 'name', 'unless', 'during', 'side', 'somewhere', 'therefore', 'therein', 'third', 'beforehand', 'six', 'why', 'often', 'various', 'eleven', 'alone', 'others', 'on', 'across', 'about', 'even', 'part', 'am', 'together', 'be', 'amount', 'less', 'these', 'throughout', 'than', 'well', 'will', 'you', 'all', 'say', 'none', 'thus', 'whenever', 'whether', 'yourself', 'myself', 'put', 'former', 'seemed', '’d', 'seems', 'almost', 'anyway', 'whole', 'anywhere', 'towards', 'was', 'ever', 'were', 'already', 'though', 'whereby', 'would', 'also', 'its', 'besides', 'had', \"'m\", 'n’t', 'it', 'whatever', 'just', 'who', 'becoming', 'whence', 'against', 'because', 'quite', 'neither', 'been', 'might', 'mine', 'front', 'further', 'then', 'really', 'call', 'last', 'fifteen', 'never', 'same', 'whoever', 'bottom', 'if', 'no', 'rather', 'that', 'under', 'onto', 'from', 'in', 'made', 'where', 'which', 'hence', 'yourselves', '’ve', 'since', 'along', 'eight', 'this', 'down', \"'s\", 'beyond', 'otherwise', '’re', 'fifty', 'each', 'per', 'least', \"'ve\", 'move', 'using', 'become', 'empty', 'formerly', 'within', 'latter', \"n't\", \"'re\", 'elsewhere', 'beside', 'is', 'became', 'into', 'those', 'whom', 'whereafter', 'both', 'before', 'via', '‘ll', 'any', 'what', 'show', 'cannot', 'first', 'five', 'latterly', 'and', 'hereupon', 'themselves', 'two', 'moreover', 'ours', 'thence', 'somehow', 'ten', 'above', 'at', 'still', 'full', 'everything', 'regarding', 'becomes', 'their', 'herself', 'sixty', '‘re', 'forty', 'her', 'must', 'here', 'get', 'make', 'nor', 'indeed', 'some', 'thru', 'due', 'itself', 'wherever', 'whither', 'up', 'nothing', 'go', 'again', 'many', 'sometimes', 'next', 'she', 'twelve', 'four', 'ca', '‘d', 'either', 'take', \"'ll\", 'three', 'by', 'anything', 'could', 'my', 'something', 'over', 'for', 'to', '‘m', 'behind', 'or', 'them', 'being', 'thereby', 'give', 'twenty', '’ll', 'without', 'whereupon', 'please', 'every', 'several', 'ourselves', 'below', 'although', 'noone', '’m', 'enough', 'around', 'more', 'should', 'with', 'few', 'always', 'as', 'now', 'off', 'an', 'other', 'anyone', 'back', 'nobody', 'not', 'there', 'we', 'own', 'thereupon', 'anyhow', 'how', 'nowhere', 'does', 'everywhere', 'of', 'top', 'see', 'very', 'him', 'may', 'while', 'yours', 'yet', 'keep', 'among', 'everyone', 'sometime', 'such', 'else', 'hereby', 'herein', 'upon', 'whose', 'nevertheless', 'one', 'n‘t', 'out', 'much', 'do', 'our', 're', 'meanwhile', 'nine', 'doing', 'perhaps', 'amongst', 'they', 'however', 'toward', 'i', 'after', 'through', '‘ve', 'has', 'another', 'did', 'once', \"'d\", '’s', 'his', 'done', 'wherein', 'namely', 'but', 'mostly', 'someone', 'have', 'me', 'are', 'except', 'a', 'the', 'seeming', 'afterwards', 'seem', 'most', 'too', 'himself', 'when'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqHJuTlCaIzW",
        "colab_type": "text"
      },
      "source": [
        "### n-grams using NLTK\n",
        "\n",
        "Spacy however does not have a built-in library to do n-gram tokenisation. This is done much easier using nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxj9a9fCaIzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQxGDJeaIzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "23050b1b-3caf-40a6-9203-33a87d99d8ba"
      },
      "source": [
        "# in the below we obtain pairs of JJ-NN\n",
        "def ngramise(sequence):\n",
        "    '''\n",
        "    Iterate over bigrams and 1,2-skip-grams.\n",
        "    '''\n",
        "    for bigram in nltk.ngrams(sequence, 2):\n",
        "        yield bigram\n",
        "    for trigram in nltk.ngrams(sequence, 3):\n",
        "        yield trigram[0], trigram[2]\n",
        "\n",
        "def jjnn_pairs(phrase):\n",
        "    '''\n",
        "    Iterate over pairs of JJ-NN.                     #jj refers to adjectives and NN refers to nouns\n",
        "    '''\n",
        "    tagged = nltk.pos_tag(nltk.word_tokenize(phrase))\n",
        "    for ngram in ngramise(tagged):\n",
        "        tokens, tags = zip(*ngram)\n",
        "        if tags == ('JJ', 'NN'):\n",
        "            yield tokens\n",
        "\n",
        "       \n",
        "for gram in jjnn_pairs(\"This is a delicious apple\"):\n",
        "    print (gram)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-26f217d4e22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjjnn_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a delicious apple\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-26f217d4e22b>\u001b[0m in \u001b[0;36mjjnn_pairs\u001b[0;34m(phrase)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mIterate\u001b[0m \u001b[0mover\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mJJ\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m                     \u001b[0;31m#jj refers to adjectives and NN refers to nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     '''\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngramise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJv6zuoTaIzc",
        "colab_type": "text"
      },
      "source": [
        "## Word embedding / Similarity\n",
        "\n",
        "A word embedding is a representation of a word, and by extension a whole language corpus, in a vector or other form of numerical mapping. This allows words to be treated numerically with word similarity represented as spatial difference in the dimensions of the word embedding mapping. The word vectors can be used as training features and will be described in greater detail in Day 2 (and also previously in Text Analytics).\n",
        "\n",
        "Example:\n",
        "\t\n",
        "With word embeddings we can understand that vector operations describe word similarity. This means that we can see vector proofs of statements such as:\n",
        "\n",
        "\tking-queen==man-woman"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNwwhxo7aIzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c65120ad-3467-4f6f-8c96-c3bc16f8110e"
      },
      "source": [
        "doc = nlp(u\"Apples and orange are similar. Hippos and lions are not.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.has_vector, token.vector_norm)  \n",
        "    print(token.vector)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apples True 24.373877\n",
            "[ 6.48012     0.4459746   0.8411504  -2.4217246  -0.81870496  0.39663044\n",
            "  4.8589735   4.484127   -3.0453458   2.6891198   1.4677203  -0.26530552\n",
            " -3.7863774  -1.2364017  -3.9447026  -0.33001122  0.88959736  0.97850204\n",
            " -1.7332234   0.3293991  -1.8573451  -0.00700772 -1.0143324   0.89394915\n",
            "  0.02612577 -0.00712687  1.8551004  -3.123006    6.8388224   0.781844\n",
            "  3.97846    -1.882559   -0.73632044  0.08226132  1.7286662   2.0666804\n",
            " -3.7042937   5.114806   -0.32460582  2.112061   -2.709859   -0.29375124\n",
            "  3.9201634  -2.1118784   5.2576485  -4.1561074  -1.9643553  -1.3030659\n",
            "  0.5393265  -2.40053    -5.5046544  -0.23191261 -1.4842712  -0.1768856\n",
            "  1.2075411   1.0816665  -0.20792592 -1.4299974  -4.481552   -2.4571939\n",
            " -0.9589004  -0.24006546  4.030328   -1.1740503   0.5638145   6.994032\n",
            " -1.0593221   1.3389046   0.22301637  1.8767828  -2.571108    0.8126358\n",
            "  1.5665478   1.6212194   0.3533219  -1.0702294   1.2023344  -0.03223666\n",
            "  1.6001799  -2.273003   -3.6745358  -2.858361   -2.2420318   1.5752151\n",
            " -1.1397029   1.3824204  -0.10399838  0.34353733  0.927062   -0.81891537\n",
            "  0.17208147 -1.3020452  -0.07435563  3.4703002  -0.7873192  -3.9238136 ]\n",
            "and True 28.449905\n",
            "[-0.8522651   1.1770346  -3.4388895  -1.0747924  -0.5544037   4.406941\n",
            "  4.577356    1.4207932  -4.308726   -2.843834   -0.8315591  -2.269878\n",
            "  1.3589044   0.9684917  -3.721991   -3.926025   -1.9364479  -0.8985468\n",
            "  3.366348    4.114743    7.315129   -2.427365   -1.6271961   1.1787633\n",
            "  1.1197656   3.0600939   1.5626397   4.329708    2.8874815  -2.0975187\n",
            "  3.4706852   0.6986804  -2.4492688  -3.2592428  -1.0077348  -0.44280404\n",
            "  0.33501992 -0.24200952 -1.8857005  -0.47408396  3.659338   -2.4412732\n",
            "  2.0055876  -0.86512554 -1.9779416   0.7405175  -4.081311    1.1458954\n",
            " -0.9006889  -4.2989483   2.5886145   0.81548846 -2.3628676   6.399465\n",
            " -0.13772988  3.0460885  -1.4157964   2.920062    5.7515044  -3.4419413\n",
            "  3.6067128  -4.855998    4.733183    1.0603884  -5.9182825   1.2131542\n",
            " -4.7637835  -0.34595048 -1.3919797  -1.2583146   8.213383   -2.9854553\n",
            "  4.7001576   3.726767    3.0519295  -0.5218743   1.7316685  -2.1045148\n",
            "  0.44443905 -2.6428742  -6.0887036   1.2802436  -2.1372635  -0.30467725\n",
            " -1.1516606  -0.02396256  0.1882452  -3.6981235  -2.5381963   0.23207879\n",
            " -0.48559475  0.8091578   0.28189522  0.6680392  -0.943079    0.84929264]\n",
            "orange True 24.579819\n",
            "[ 0.02983025  0.9792412  -1.5905881  -3.2218606  -2.7757533   1.671694\n",
            "  2.4070098   3.6794453  -1.8840059   4.83018    -1.8620778   3.0878968\n",
            " -1.252341    0.08513975 -2.6014042   1.5796175   1.7192839  -0.9093393\n",
            "  4.271901    3.8137536  -0.3733943  -4.730178    0.16032553  0.06507908\n",
            "  4.310774    1.7909567   1.0085683  -2.9088454   0.21283561  2.7261643\n",
            " -1.1004847  -3.4482224   0.18281904  1.511656    0.3698089   0.6484921\n",
            "  0.38435954  4.92533    -2.2571487  -1.0142549  -2.4713016   5.320436\n",
            "  2.4201527  -2.1702175  -3.0234616  -1.6729205  -5.339865   -1.6775639\n",
            " -2.8281944  -2.992245   -4.585133   -0.9724352  -2.438621    0.28241163\n",
            " -0.6027328   0.1168687  -0.43064708  4.285986    1.1044049   2.0122192\n",
            " -0.10112536 -1.8088094  -0.36012876 -0.01742566 -2.0867734   3.2576504\n",
            " -1.1701423  -1.1376745  -0.15587926 -1.1853389   1.3390212  -0.56528\n",
            "  1.2515714   0.8715321   2.741329    0.04943526 -0.4953609  -0.19277066\n",
            " -2.2734356  -1.1395197  -0.89346325 -1.5702118  -1.6445273  10.70842\n",
            " -2.5369244   0.57570326  2.5002337  -0.18021932  3.1387777   1.8675245\n",
            "  0.32889086 -2.8169513  -2.6176648   0.5315999   2.139114    0.11168107]\n",
            "are True 28.345718\n",
            "[-9.9731350e-01 -5.2390790e-01  1.5169919e-01  5.5188107e+00\n",
            "  1.4793983e+00 -1.9073999e-01  1.3638391e+00  3.6499777e+00\n",
            " -2.4161720e+00 -3.8963563e+00 -3.2547600e+00 -5.4674828e-01\n",
            "  3.7515712e+00  2.5188017e+00 -5.0637156e-01 -3.2832360e+00\n",
            " -2.7460587e+00  1.8061846e-01 -3.6600618e+00 -1.5560665e+00\n",
            " -3.0027747e+00 -2.9233725e+00  2.4441266e+00  4.6745330e-01\n",
            " -1.4364249e-01 -1.0831034e+00 -5.6122506e-01 -1.3190365e+00\n",
            " -5.0252194e+00  5.1230044e+00  8.4849238e-01  2.2651293e+00\n",
            "  8.1628084e-02  2.5320966e+00  3.2757065e+00  5.3505497e+00\n",
            " -5.2011973e-01  1.0286595e+00 -1.6493385e+00  1.4059941e+00\n",
            " -5.9341488e+00  2.3542168e+00  1.8576629e+00 -1.8325830e+00\n",
            " -3.1486046e+00  1.5744354e+00 -1.8748685e+00 -4.4282746e+00\n",
            " -3.2812526e+00 -2.3968337e+00  1.0278194e+00 -2.2418575e+00\n",
            "  5.8311540e-01  7.4610729e+00  6.7163038e+00 -2.7685020e+00\n",
            "  1.7313820e+00  6.7563045e-01 -1.4340858e+00  1.2661315e+00\n",
            "  8.2870287e-01  3.3644469e+00  1.9721998e+00  9.1545649e+00\n",
            " -2.6707802e+00  3.4266455e+00  3.8064775e+00  3.3150637e+00\n",
            "  5.3781396e-01  8.9087534e-01  4.4080597e-01 -4.3159657e+00\n",
            " -3.4805932e+00  2.4291103e+00 -1.6241705e+00 -4.6066046e-03\n",
            " -4.1326404e+00 -4.0950756e+00  8.1377625e-02  1.7765806e+00\n",
            "  2.7910285e+00  1.9720730e+00  1.4912145e+00  2.0828991e+00\n",
            " -3.7940745e+00 -4.5837545e+00  1.1773252e+00 -2.2031884e+00\n",
            " -1.9424832e+00 -3.0873504e-01  2.0333226e+00 -1.9195961e+00\n",
            "  1.9055361e-01 -1.0697862e+00 -3.2130489e+00 -2.2660122e+00]\n",
            "similar True 27.723778\n",
            "[-3.1582775  -1.8306707  -2.6598444  -4.1660047   1.1836472   4.244979\n",
            "  4.8207107   2.833984    1.2351581  -2.1639752   4.4984026   3.286688\n",
            " -1.9728229  -3.9461517  -0.48345575  6.7901316  -0.5566061  -1.640636\n",
            " -3.6814306  -0.43600237 -1.4068165  -3.5004277   1.3388753   4.4659796\n",
            "  2.3869686   0.75412375  4.260275   -3.4763114  -2.6177075   4.724592\n",
            "  1.0710274   1.6815034  -1.3289057  -0.2974838  -0.10666484 -0.34310454\n",
            "  1.5855279  -3.4099152  -3.6777582   0.7329481   4.126271    0.08896476\n",
            "  1.8669118  -1.9449798  -4.538348   -2.9860969   0.37586784 -2.717562\n",
            " -2.7564175  -4.5463376  -4.0466566  -2.2444932  -0.7013709  -1.2588394\n",
            " -0.8501529  -0.25565037  2.2223144   3.0054514  -2.5589867   1.5712636\n",
            " -2.3512554  -0.33518708 -2.0721586   0.65906715 -0.93414074 -0.21549079\n",
            "  4.6484375  -1.3971577  -4.449519    0.41000262 -0.52054447  1.1319375\n",
            " -3.4263997   2.0192847   2.8490744   4.4323287   6.145389    2.6717055\n",
            "  5.9810953   2.3285422   1.2996662  -3.7288842  -2.0396626   1.5787596\n",
            " -0.586606   -0.8749541   3.644186    2.0704236   2.6287262  -2.3077161\n",
            "  3.9479446   1.2525558  -4.2818937  -1.9894758  -1.9538646  -2.2828496 ]\n",
            ". True 26.09941\n",
            "[ 1.492288   -3.5149355   3.3725588   2.9776626   0.77504295 -2.9528196\n",
            "  4.9963617   1.2659783  -1.9768672  -0.877586    4.141018    2.3749633\n",
            "  0.4638908  -1.6959834  -3.6310887   2.0741298  -0.18188184 -1.0202278\n",
            "  5.2153077   1.5571139   0.05417144  2.1995757   0.5702745   1.1406554\n",
            " -2.7435539  -0.09009591 -0.581053   -1.00718     1.2926686  -0.6751036\n",
            "  0.04192226 -0.49312937 -4.5006237  -3.5583627  -0.39453876  0.5334605\n",
            " -0.06112967 -3.1710556  -3.3249054   2.5463111  -0.14653897  1.6101937\n",
            "  0.8176913   2.9026973   1.5327675   3.7315192   3.1153283  -0.46226045\n",
            "  0.83248955  1.68392    -1.0751072   2.087595   -2.3750708   0.68015534\n",
            " -6.667874    2.5607588   1.9487176   1.0232843  -2.7561085   0.38295695\n",
            "  2.8303537  -0.41319966  4.14027    -1.0193934  -3.4849524  -3.987043\n",
            "  0.22840345  0.9776629  -2.9841511  -1.7539852  -0.7268776   1.5955582\n",
            "  5.2863693  -2.4765282  -2.0034587   3.7358503   1.6424699   0.03976667\n",
            " -1.2730521  -2.678806    4.141452    0.20355567 -2.6105614   5.8745193\n",
            "  3.9254518  -2.0084991  -2.3713286  -0.8939725  -5.806041   -3.6725848\n",
            " -4.356402    0.34448326  5.380688   -4.322505   -1.8421372  -0.81977975]\n",
            "Hippos True 24.945618\n",
            "[ 6.574659   -2.983436   -1.6029273  -1.9718711  -1.8254592   2.331802\n",
            "  0.68417954 -0.45334125 -1.5618993  -0.20365798  0.74742854  0.37420988\n",
            " -5.413686   -2.1333225  -0.98534214 -2.4205797   5.270395    1.5797436\n",
            "  0.0893282   5.9552016  -4.152948   -4.455038    0.85154295  3.5986218\n",
            " -0.64031076  2.1356091  -0.82066077 -2.0920882   2.9570312   6.5217524\n",
            "  4.896229   -1.4308887  -2.3212078  -0.6377169   0.5098746   0.10293031\n",
            "  1.0475912   1.224255   -0.4301504  -1.1027509   0.9618504   1.2385542\n",
            "  0.7493092   0.70231616 -1.1307615   0.19600165 -3.519142    1.2074647\n",
            "  1.8089201  -1.8517525  -5.479446    0.03027299 -4.0170937   1.2890862\n",
            " -0.11849749 -1.8476918  -0.47623935 -1.5931712  -0.8145235  -2.9334426\n",
            " -0.32176197  1.3688254   1.9968178  -3.164048    0.46642637  3.2842655\n",
            "  2.7039983  -0.20518982 -2.3571837   1.1102159   1.7073846   0.5883292\n",
            "  2.5614889   3.3917172  -1.3999891   3.5420165   0.7368921   1.9905772\n",
            "  1.7562938  -4.2145467  -2.6353157  -1.142446   -4.897813    3.809466\n",
            "  1.3932289  -1.6655828  -1.1200091  -3.420817    3.9177294  -1.1616094\n",
            " -0.89936495 -0.5592064  -0.1819303   2.9466097  -2.2762713   3.063542  ]\n",
            "and True 27.845913\n",
            "[ 1.159108    1.1177855  -3.8190084  -3.7353783  -1.2117493   1.3105385\n",
            "  4.8948946   0.969188   -2.7747478  -3.6005917   0.60760325 -1.773907\n",
            " -2.5150614  -1.9088259  -3.2612896  -2.6801744  -0.79157174 -0.10165113\n",
            "  1.8243316   2.8753295   8.907541   -3.2656898  -0.7592244   1.9361085\n",
            " -0.45388782  1.8938876  -0.72683996  4.7696958   1.9081388  -2.7800517\n",
            "  2.9681132  -1.2695439  -2.6217947  -3.8848062   1.7511492  -0.03289926\n",
            "  1.0562791  -0.5163427  -0.88168764  1.6290457   3.935751    0.6857242\n",
            "  2.2103329  -0.24725437 -1.5248858   3.1576197  -3.1977067   4.1512446\n",
            " -1.4108093  -2.8263402   0.67996764  2.7221487  -1.8120618   5.789277\n",
            " -2.3276353   2.661806   -0.75556624  2.1475327   3.6488488  -2.4511304\n",
            "  2.1756592  -6.4582744   5.695646   -0.33768743 -5.203588    0.68344384\n",
            " -4.032413    2.2900522   1.0728759  -0.8221197   7.6114225  -2.1123128\n",
            "  4.9258356   0.48224032  2.3173642  -2.5934381   2.0903437   0.9849864\n",
            "  0.8089489  -1.8712709  -3.350874    2.206129   -4.4416456  -1.1908755\n",
            " -1.3629656  -1.6969814   1.9885113  -3.2134657  -2.913816   -2.519514\n",
            " -2.1721458   1.1458743  -2.8640788  -0.30534917  0.5681687   1.5844939 ]\n",
            "lions True 26.324625\n",
            "[ 4.333522    1.7688106  -1.2927165  -3.0131955  -3.0706496   0.22903204\n",
            "  3.474912   -0.23420441 -0.57649475  5.52027     1.0385232   0.85838556\n",
            " -2.3024757  -1.9740939  -3.882471    1.6920907   2.8503718  -1.6541052\n",
            "  6.823563    1.6958      0.19105434  3.2472024  -2.638507   -1.3942633\n",
            "  3.1336095   0.91307175  1.4554715  -1.163235    0.14788604 -0.59492844\n",
            "  1.9270885  -3.8105297  -0.786089    0.8550809   0.65802604 -2.9842541\n",
            "  2.0267634   6.867897   -1.0783634   2.3119078  -3.9653637   4.766795\n",
            "  3.005423   -0.7937236   4.865651   -0.4759239  -3.0986161   0.50204754\n",
            " -1.2990168  -0.8770201  -5.6711574   0.3551486   1.1619368  -2.9615712\n",
            "  0.93323386 -1.4418721  -1.254942    3.361782    1.3049808  -0.61794484\n",
            "  3.9950082   0.47856975  1.2257881  -1.7696961   2.2946632   7.6756115\n",
            " -4.443536    3.862372   -2.2746723   0.09047946 -1.7903076  -0.49983513\n",
            "  0.8678005  -1.8827188   0.1554685  -2.8064384   1.5673926  -3.2306652\n",
            " -2.3038194  -2.32881    -2.854674   -3.3969386  -1.3594798   5.9049287\n",
            " -1.6316252  -0.76841813 -0.43949085 -0.02334854  1.1620357  -0.6561427\n",
            " -2.1525261  -2.401219   -2.4926724  -0.20559967  0.94438994 -3.166037  ]\n",
            "are True 28.157677\n",
            "[-4.3215987e-01 -4.1069627e-01  2.3306656e+00  3.8942766e+00\n",
            " -1.1519822e+00  1.0765792e+00  3.0266199e+00  7.5156040e+00\n",
            " -2.3564014e+00 -2.6929898e+00 -6.0647506e-01 -2.1448903e+00\n",
            "  3.7462597e+00  5.2872479e-01  4.4470605e-01 -1.6039674e+00\n",
            " -3.3749976e+00  6.4285642e-01 -3.9218261e+00 -2.2902150e+00\n",
            " -5.0514565e+00 -4.1537561e+00  3.2998033e+00 -1.7441572e+00\n",
            " -1.1742213e+00 -7.1140492e-01 -3.7282152e+00 -2.0234971e+00\n",
            " -3.3930123e+00  2.7101421e+00 -1.4067426e+00  1.6713424e+00\n",
            " -1.0471627e+00  4.3386254e+00  6.3403125e+00  3.1780014e+00\n",
            " -9.4494700e-01 -1.2102705e+00 -2.3640187e+00 -1.2582113e+00\n",
            " -2.8324692e+00 -4.1817087e-01 -3.2741475e-01 -9.1414475e-01\n",
            " -3.4952164e+00 -7.2787762e-01 -2.2660048e+00 -9.8022699e-01\n",
            " -2.8192616e+00 -1.9056145e+00  1.6685736e+00 -3.3355916e+00\n",
            "  2.9095060e-01  4.7210946e+00  2.8840098e+00 -2.2400663e+00\n",
            "  3.5341072e+00  1.2933284e+00  1.3263104e+00  5.3359156e+00\n",
            "  2.5330424e-01  5.3761082e+00 -4.2956942e-01  6.7456617e+00\n",
            " -1.5080658e+00  4.0502853e+00  3.2437859e+00  1.5492101e+00\n",
            " -6.6404629e-01 -1.7173271e+00  5.7959852e+00 -4.8034859e+00\n",
            " -5.0542035e+00  2.5629872e-01 -1.9062960e+00  2.3525491e+00\n",
            " -1.2188853e+00 -2.0314798e+00 -6.9128740e-01  6.6942149e-01\n",
            "  5.6009083e+00  2.4121876e+00  3.2416451e+00  4.3770671e-03\n",
            "  3.1537261e+00 -3.4328990e+00  7.3212659e-01 -3.3472939e+00\n",
            " -2.3302207e+00  9.5131451e-01  8.7752676e-01 -2.7654443e+00\n",
            "  1.0196508e+00  1.3760092e+00 -3.5988758e+00 -1.8741035e+00]\n",
            "not True 26.297699\n",
            "[-4.4913807   0.27710468 -4.848268   -3.5285323  -1.3785979   4.2348666\n",
            " -0.9012834   5.7233253   1.544069    3.9008007   0.04673964  2.0197973\n",
            " -3.1095142   1.5407232  -2.2909236   1.1993425  -0.89195746 -1.397028\n",
            "  1.9503278  -0.2790184  -3.761876    3.573827    3.31601     0.84764135\n",
            "  6.5957994   0.98533875  6.2360306   3.6472297   0.24681425 -1.5472586\n",
            "  0.7285264  -1.5646201  -5.090316    4.639006    2.0570772  -2.4064481\n",
            " -0.3851143   0.1534077  -1.5849588  -1.5456387   1.3807967  -1.4737206\n",
            "  0.02937728  3.7039356  -0.87776816  3.2381911  -0.62589055 -0.20969462\n",
            " -1.0528355  -1.2912498  -2.394327   -4.454845   -1.8586133  -2.3089492\n",
            "  0.38031211 -0.15393603 -0.84156835  4.8950987  -3.6343796   5.6131945\n",
            "  0.5525516   1.0635548  -4.5199056   5.547385   -0.3070558   0.8488997\n",
            "  1.2558639   1.8533137  -1.8179623   0.41920263  0.15107465  0.01715213\n",
            " -2.9111943  -1.2783489  -0.788097   -2.2773163   2.5221844  -1.3740771\n",
            "  3.9666197   2.1092885  -2.6004035   0.14392361 -1.2367518  -0.6778184\n",
            " -3.1257126  -1.3631446   1.6175739  -2.9761121   0.07350716 -0.9666771\n",
            "  1.2180195   4.412037   -3.3374736   1.4972236  -3.7847452  -2.7714806 ]\n",
            ". True 26.973852\n",
            "[ 0.20317388  1.2619232   1.429136    3.935287   -0.06809443 -0.3671707\n",
            "  1.9181049   3.0582902  -1.2638098  -2.5281603   4.699063    1.1131935\n",
            " -0.08408242 -2.8581827  -2.74454     4.891388    0.2924217  -1.4690473\n",
            "  4.8310976   2.0802426  -1.0533245   3.0041623  -1.8885267  -0.3888476\n",
            " -2.5817506   1.8371637  -0.72331893  0.41646934  1.354242   -1.7321413\n",
            " -1.3404839  -0.71379566 -4.0822043  -3.0244594   1.5193632  -1.2455218\n",
            " -0.39287233 -2.4194221  -3.2277126   1.0813651   0.43274546 -2.0706162\n",
            " -1.3142811   4.8068514  -1.420104   -0.2807328   4.2056427   0.90162444\n",
            " -0.71439433  5.0926056  -0.612602   -1.7593905  -1.630942   -1.0391097\n",
            " -1.8315728   1.8333905   1.4779222  -1.2802545   1.3082397  -1.1065233\n",
            "  1.7986219  -0.6820919   1.5728483  -0.20529509 -1.87916    -2.1247225\n",
            "  1.5609128   0.73324907 -4.214308   -1.5676677  -5.6836567  -0.9604578\n",
            "  2.6022563  -1.8467131  -1.6979511   8.666398    3.76974    -0.13583726\n",
            " -1.3602854   0.09707925  6.941833    0.53646016 -5.2765875   6.622088\n",
            "  6.3129444  -4.3744373  -1.8308408   0.26040822 -0.5035516  -4.134458\n",
            " -1.5870394  -2.1598916   5.066968   -2.3067687  -2.051003   -1.0556362 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HbAr_iKaIzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e26ee3b1-1b61-48b2-83a2-ac202e959e9a"
      },
      "source": [
        "# add stop words\n",
        "nlp.Defaults.stop_words -= {\"are\", \".\", \"and\"}\n",
        "\n",
        "# Finding the similarity between words    ##Cosine similarity\n",
        "\n",
        "for token1 in doc:\n",
        "    for token2 in doc:\n",
        "        if not token1.is_stop and not token2.is_stop and token1 != token2:\n",
        "            print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Apples orange 0.33165458\n",
            "Apples similar 0.096144706\n",
            "Apples . 0.021095317\n",
            "Apples Hippos 0.4646771\n",
            "Apples lions 0.5326731\n",
            "Apples . -0.058706053\n",
            "orange Apples 0.33165458\n",
            "orange similar 0.24862732\n",
            "orange . -0.039233293\n",
            "orange Hippos 0.35312775\n",
            "orange lions 0.5714338\n",
            "orange . -0.06659413\n",
            "similar Apples 0.096144706\n",
            "similar orange 0.24862732\n",
            "similar . -0.0061813476\n",
            "similar Hippos 0.20505269\n",
            "similar lions -0.0695914\n",
            "similar . 0.118038565\n",
            ". Apples 0.021095317\n",
            ". orange -0.039233293\n",
            ". similar -0.0061813476\n",
            ". Hippos 0.10610138\n",
            ". lions 0.07583006\n",
            ". . 1.0\n",
            "Hippos Apples 0.4646771\n",
            "Hippos orange 0.35312775\n",
            "Hippos similar 0.20505269\n",
            "Hippos . 0.10610138\n",
            "Hippos lions 0.2377463\n",
            "Hippos . 0.083698876\n",
            "lions Apples 0.5326731\n",
            "lions orange 0.5714338\n",
            "lions similar -0.0695914\n",
            "lions . 0.07583006\n",
            "lions Hippos 0.2377463\n",
            "lions . 0.031110972\n",
            ". Apples -0.058706053\n",
            ". orange -0.06659413\n",
            ". similar 0.118038565\n",
            ". . 1.0\n",
            ". Hippos 0.083698876\n",
            ". lions 0.031110972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HQZx_nyaIzh",
        "colab_type": "text"
      },
      "source": [
        "## Syntactic dependencies\n",
        "\n",
        "#### What are syntactic dependencies?\n",
        "\n",
        "We have the speech tags and we have all of the tokens in a sentence, but how do we relate the two to uncover the syntax in a sentence? Syntactic dependencies describe how each type of word relates to each other in a sentence, this is important in NLP in order to extract structure and understand grammar in plain text.\n",
        "\n",
        "Example:\n",
        "\n",
        "<img src=\"images/syntax-dependencies-oliver.png\" align=\"left\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RggvaMHFaIzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08810053-cd47-4552-b1ff-41728fd6dfd8"
      },
      "source": [
        "doc = nlp(u'The three monkeys ate a bunch of delicious bananas.')\n",
        "print(doc)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The three monkeys ate a bunch of delicious bananas.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_WINmdGaIzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d08379cc-1a62-4cc0-d867-11068f8d524a"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4QSwxogaIzq",
        "colab_type": "text"
      },
      "source": [
        "#### Using WordNet to change sentences expression\n",
        "\n",
        "This is similar to Valentino, in using WordNet to change expressions to have a higher valence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2_0cghZaIzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from spacy_wordnet.wornet_annotator import WordnetAnnotator \n",
        "import spacy_wordnet\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "# nltk.download(\"wordnet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IlPHGFyaIzv",
        "colab_type": "code",
        "colab": {},
        "outputId": "12bc3cfb-aed2-4347-f5ef-afd223e32003"
      },
      "source": [
        "if \"WordnetAnnotator\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n",
        "print(\"after\", nlp.pipe_names)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after ['tagger', 'WordnetAnnotator', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67jQzyfaaIzx",
        "colab_type": "code",
        "colab": {},
        "outputId": "4bbb1909-895d-44a5-936e-545e706da33f"
      },
      "source": [
        "token = nlp(\"beautiful\")[0]  # try with other words like 'bank' etc\n",
        "token_ = token._.wordnet.synsets()\n",
        "token_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('beautiful.a.01'), Synset('beautiful.s.02')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOP6gRUKaIzz",
        "colab_type": "code",
        "colab": {},
        "outputId": "9f43b75e-a6c2-4a09-8454-c63eb76cd669"
      },
      "source": [
        "token._.wordnet.lemmas()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('beautiful.a.01.beautiful'), Lemma('beautiful.s.02.beautiful')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBRokaTCaIz1",
        "colab_type": "code",
        "colab": {},
        "outputId": "566c9e1f-b7da-4b30-e38f-87cb56edbc9d"
      },
      "source": [
        "token._.wordnet.wordnet_domains()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['philosophy',\n",
              " 'quality',\n",
              " 'mythology',\n",
              " 'painting',\n",
              " 'plants',\n",
              " 'art',\n",
              " 'body_care',\n",
              " 'person',\n",
              " 'fashion',\n",
              " 'graphic_arts',\n",
              " 'drawing',\n",
              " 'archaeology',\n",
              " 'meteorology']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70sz9nQdaIz3",
        "colab_type": "code",
        "colab": {},
        "outputId": "d76f4b28-1960-4a53-9feb-d5b79abee1be"
      },
      "source": [
        "domains = [\"painting\",\"art\",\"graphic_arts\"]\n",
        "sentence = nlp(\"This beautiful picture looks awesome.\")\n",
        "\n",
        "enriched_sent = []\n",
        " \n",
        "for token in sentence:\n",
        "    # get synsets within the desired domains\n",
        "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
        "    synsets = token._.wordnet.synsets()\n",
        "    if synsets and token.pos == spacy.parts_of_speech.ADJ:\n",
        "       lemmas_for_synset = []\n",
        " \n",
        "       for s in synsets:\n",
        "           # get synset variants and add to the enriched sentence\n",
        "           lemmas_for_synset.extend(s.lemma_names())\n",
        "           enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
        "    else:\n",
        "        enriched_sent.append(token.text)\n",
        " \n",
        "print(\" \".join(enriched_sent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This (beautiful) (beautiful) picture looks (amazing|awe-inspiring|awesome|awful|awing) .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkYKG_QLaIz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Placeholder only for reference\n",
        "\n",
        "### Multi-threaded generator\n",
        "#texts = [u'One document.', u'...', u'Lots of documents']\n",
        "# .pipe streams input, and produces streaming output\n",
        "#iter_texts = (texts[i % 3] for i in range(1000))\n",
        "#for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=4)):\n",
        "#    assert doc.is_parsed\n",
        "#    if i == 100:\n",
        "#        break\n",
        "\n",
        "#Efficient binary serialization\n",
        "#from spacy.tokens.doc import Doc\n",
        "#byte_string = doc.to_bytes()\n",
        "#open('moby_dick.bin', 'wb').write(byte_string)\n",
        "#for byte_string in Doc.read_bytes(open('moby_dick.bin', 'rb')):\n",
        "#   doc = Doc(nlp.vocab)\n",
        "#   doc.from_bytes(byte_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bzxEIYPaIz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}